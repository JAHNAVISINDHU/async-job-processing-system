```md
# Async Job Processing System âœ…

[![Unit tests](https://github.com/JAHNAVISINDHU/async-job-processing-system/actions/workflows/ci.yml/badge.svg)](https://github.com/JAHNAVISINDHU/async-job-processing-system/actions/workflows/ci.yml)
[![Integration tests](https://github.com/JAHNAVISINDHU/async-job-processing-system/actions/workflows/integration.yml/badge.svg)](https://github.com/JAHNAVISINDHU/async-job-processing-system/actions/workflows/integration.yml)

This repository contains a background job processing system with a REST API, persistent job metadata, Redis-backed queues, and an email/CSV worker pipeline.

---

## Overview

This system shows how to offload long-running work (CSV exports, email sending) from a web API to a background worker using queues and retries.

**Tech stack:**

- Node.js (Express) for the REST API
- PostgreSQL for persistent job metadata
- Redis for queueing and priority handling
- MailHog as a mock SMTP server
- Docker + Docker Compose for one-command orchestration

---

## Features

- `POST /jobs` to create and enqueue jobs (`CSV_EXPORT`, `EMAIL_SEND`)
- `GET /jobs/:id` to fetch job status and metadata
- Admin endpoints under `/admin/*` to list, inspect, and manage jobs
- Worker process that listens to two queues: `high_priority` and `default`
- Priority handling: worker uses Redis `BLPOP` on `queue:high_priority` and `queue:default` so high-priority jobs are processed first
- Retries: jobs are retried up to 3 times before being marked as failed
- CSV export jobs write artifacts to a shared `./output/` directory
- Email jobs send messages via MailHog (no real emails sent)
- One-command startup with `docker-compose up` for all services

---

## Repository

GitHub:

```text
https://github.com/JAHNAVISINDHU/async-job-processing-system.git
```

Clone:

```bash
git clone https://github.com/JAHNAVISINDHU/async-job-processing-system.git
cd async-job-processing-system
```

---

## Quick Start

1. Ensure Docker and Docker Compose are installed.
2. Copy the environment example:

   ```bash
   cp .env.example .env
   # Adjust variables if needed (ports, DB credentials, ADMIN_USER/ADMIN_PASS, etc.)
   ```

3. Start the full stack:

   ```bash
   docker-compose up --build
   ```

4. Wait until the API health endpoint returns OK:

   ```text
   http://localhost:3000/health
   ```

5. Open MailHog UI to view captured emails:

   ```text
   http://localhost:8025
   ```

---

## Endpoints and URLs ðŸ”—

When running locally via `docker-compose up --build`:

- **API health**

  - `GET http://localhost:3000/health` â€“ readiness check used by health checks and integration tests.

- **API base**

  - Base URL: `http://localhost:3000`

  - `POST /jobs` â€“ create a job (CSV or email):

    Request body:

    ```json
    {
      "type": "CSV_EXPORT" | "EMAIL_SEND",
      "priority": "high" | "default",
      "payload": { }
    }
    ```

    Response:

    ```json
    { "jobId": "<uuid>" }
    ```

  - `GET /jobs/:id` â€“ fetch job details and status for a given `jobId`.

  - Admin endpoints:

    - `GET /admin/jobs` â€“ list jobs
    - `GET /admin/failed` â€“ list failed jobs
    - `POST /admin/jobs/:id/retry` â€“ retry a specific job
    - `POST /admin/jobs/:id/dlq` â€“ move a job to the dead-letter queue

- **Admin UI (web)**

  - `http://localhost:3000/admin/ui` â€“ minimal management UI for failed jobs.
  - Protected with HTTP Basic Auth when `ADMIN_USER` and `ADMIN_PASS` are set in `.env`.

- **MailHog**

  - UI: `http://localhost:8025`
  - Messages API: `http://localhost:8025/api/v2/messages`

- **PostgreSQL (inside Compose)**

  - Host: `db:5432`
  - Example:

    ```bash
    docker-compose exec db psql -U user -d jobs_db -c "SELECT * FROM jobs;"
    ```

- **Redis CLI (inside Compose)**

  ```bash
  docker-compose exec redis redis-cli
  LLEN queue:high_priority
  LRANGE queue:default 0 -1
  ```

- **Output directory (host)**

  - `./output` â€“ CSV files generated by `CSV_EXPORT` jobs, named `{jobId}.csv`.

---

## API Reference

### POST /jobs

- **Description:** Create and enqueue a new job.
- **Body:**

  ```json
  {
    "type": "CSV_EXPORT" | "EMAIL_SEND",
    "priority": "high" | "default",
    "payload": { }
  }
  ```

- **Response:**

  ```json
  { "jobId": "<uuid>" }
  ```

### GET /jobs/:id

- **Description:** Retrieve job metadata and current status (e.g., queued, processing, completed, failed).

---

## Job Types

### CSV_EXPORT

- **Payload:**

  ```json
  {
    "data": [
      { "id": 1, "name": "Alice" },
      { "id": 2, "name": "Bob" }
    ]
  }
  ```

- **Behavior:**

  - Writes a CSV file to `/usr/src/app/output/{jobId}.csv` inside the container.
  - This path is mapped to `./output` on the host.

### EMAIL_SEND

- **Payload:**

  ```json
  {
    "to": "user@example.com",
    "subject": "Subject line",
    "body": "Message body"
  }
  ```

- **Behavior:**

  - Sends an email via MailHog SMTP.
  - Messages appear in the MailHog UI (`http://localhost:8025`).

---

## Background Worker Behavior

- Listens on Redis lists `queue:high_priority` and `queue:default`.
- Uses `BLPOP` so `high_priority` jobs are preferred whenever available.
- Each job can be retried up to 3 times before being marked as failed.
- Failed jobs can be retried or moved to a dead-letter queue using admin endpoints.

---

## Project Structure

- `docker-compose.yml` â€“ orchestrates API, worker, PostgreSQL, Redis, and MailHog containers.
- `seeds/01_init.sql` â€“ DB init script to create the `jobs` table and related schema.
- `src/server.js` â€“ Express API server (job and admin endpoints).
- `src/worker.js` â€“ background worker (pulls jobs from Redis and processes them).
- `output/` â€“ shared volume for generated CSV artifacts.

---

## Integration Tests âœ…

Integration tests exercise the full stack (API â†’ worker â†’ DB/Redis â†’ output/MailHog).

1. Install dependencies:

   ```bash
   npm install
   ```

2. Run integration tests:

   ```bash
   npm run test:integration
   ```

The tests will:

- Create a `CSV_EXPORT` job and an `EMAIL_SEND` job via the API.
- Wait for both jobs to complete.
- Verify the CSV file exists at `./output/{jobId}.csv`.
- Query MailHog to confirm the email was delivered.

---

## One-Command Smoke Test âš¡

A single-command smoke test validates the entire stack end-to-end.

1. Install dependencies:

   ```bash
   npm install
   ```

2. Run the smoke test:

   ```bash
   npm run smoke
   ```

This command will:

1. Run `docker-compose up --build -d` to start all services.
2. Wait until `http://localhost:3000/health` is healthy.
3. Post an `EMAIL_SEND` job and wait for completion, then check MailHog for the message.
4. Post a `CSV_EXPORT` job and wait for completion, then verify the CSV file is present in `./output`.

---

## Admin UI, CI, and Secrets

- Integration tests include a headless browser test for the Admin UI at `/admin/ui`.
- When `ADMIN_USER` and `ADMIN_PASS` are configured, the test authenticates via HTTP Basic Auth.

### CI Setup (GitHub Actions)

To run integration tests in CI with admin protection enabled:

1. Add the following repository secrets:

   - `ADMIN_USER`
   - `ADMIN_PASS`

2. In GitHub: Repository â†’ Settings â†’ Secrets and variables â†’ Actions â†’ **New repository secret**.

The integration workflow will inject these into the `.env` file before starting the stack so the admin UI is protected during CI runs.

If you do **not** set these secrets:

- The workflow still runs.
- The admin UI test will be skipped in CI or run without auth locally.
- Adding the secrets ensures the E2E admin UI test verifies Basic Auth protection.
---

## Badges Note

The CI badges at the top of this README are configured for:

- User: `JAHNAVISINDHU`
- Repository: `async-job-processing-system`

